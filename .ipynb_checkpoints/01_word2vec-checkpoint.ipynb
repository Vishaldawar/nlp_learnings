{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a83c32-80da-4bf3-85d5-6d721e916fef",
   "metadata": {},
   "source": [
    "## NLP - Lesson 2\n",
    "\n",
    "In Natural Language Processing, the idea is to learn patterns and form insights from textual data using a computer. But as computers cannot understand the text directly, we have to convert the text into numerical data which then can be used as an input to traditional and modern models. Machine learning algorithms can handle any dimension of textual data when converted to numerical data using techniques like word embeddings, for example word2vec. This notebook is a continuation of [00_vectorizer.ipynb](https://github.com/Vishaldawar/nlp_learnings/blob/main/00_vectorizer.ipynb) where we discussed basic string to vector conversion techniques like tf-idf and count vectorizer. In this notebook, we will build up on these techniques by implementing a word2vec model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cbdcef-2e28-41a5-9345-b21603f692a1",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb584e82-86ba-4567-bf6f-d45cfddef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388e75a-bf52-4caf-9905-f48e7674616e",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Word2Vec is an approach to convert textual data into word embeddings. This is done by using contextual words (or words around the input word) and making the machine predict the surrounding words. This is achieved by using a neural network with a hidden layer. The final output of the neural network is a softmax layer with probabilities of words in our corpus to be the next word. The embeddings however are calculated by as the neuron values in the hidden layer.\n",
    "\n",
    "Let us consider this sample text extracted from wikipedia to be a corpus for our implementation of word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da666f9b-645f-4826-92cf-0bff51bdd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d95bc-6af6-4767-8f4e-8fa9e947550e",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "We need to tokenize the text to get tokens for each. After that, we will create word to index and index to word mapping dictionaries for finding out the right word corresponding to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f7d965-d624-4a22-b187-54d7e24ea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1e7414-bcee-4948-bbeb-ff33b603daf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'algorithms',\n",
       " 'that',\n",
       " 'improve',\n",
       " 'automatically',\n",
       " 'through',\n",
       " 'experience',\n",
       " 'it',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'build',\n",
       " 'a',\n",
       " 'mathematical',\n",
       " 'model',\n",
       " 'based',\n",
       " 'on',\n",
       " 'sample',\n",
       " 'data',\n",
       " 'known',\n",
       " 'as',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'predictions',\n",
       " 'or',\n",
       " 'decisions',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'are',\n",
       " 'used',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'email',\n",
       " 'filtering',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'where',\n",
       " 'it',\n",
       " 'is',\n",
       " 'difficult',\n",
       " 'or',\n",
       " 'infeasible',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'conventional',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'the',\n",
       " 'needed',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4298d1b-2fda-41a5-b98e-d3d02837a08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29084ab7-5a45-4a80-bada-e632373a20dd",
   "metadata": {},
   "source": [
    "We see there are some repeated words so we will have to use a unique set of tokens for the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa030a3c-23e2-4a19-a611-a7ae98459d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 0,\n",
       " 'improve': 1,\n",
       " 'where': 2,\n",
       " 'that': 3,\n",
       " 'develop': 4,\n",
       " 'a': 5,\n",
       " 'perform': 6,\n",
       " 'so': 7,\n",
       " 'mathematical': 8,\n",
       " 'training': 9,\n",
       " 'are': 10,\n",
       " 'learning': 11,\n",
       " 'applications': 12,\n",
       " 'to': 13,\n",
       " 'used': 14,\n",
       " 'sample': 15,\n",
       " 'tasks': 16,\n",
       " 'through': 17,\n",
       " 'intelligence': 18,\n",
       " 'order': 19,\n",
       " 'conventional': 20,\n",
       " 'or': 21,\n",
       " 'without': 22,\n",
       " 'based': 23,\n",
       " 'predictions': 24,\n",
       " 'the': 25,\n",
       " 'on': 26,\n",
       " 'decisions': 27,\n",
       " 'explicitly': 28,\n",
       " 'is': 29,\n",
       " 'vision': 30,\n",
       " 'as': 31,\n",
       " 'needed': 32,\n",
       " 'of': 33,\n",
       " 'filtering': 34,\n",
       " 'being': 35,\n",
       " 'it': 36,\n",
       " 'algorithms': 37,\n",
       " 'subset': 38,\n",
       " 'machine': 39,\n",
       " 'build': 40,\n",
       " 'data': 41,\n",
       " 'experience': 42,\n",
       " 'programmed': 43,\n",
       " 'do': 44,\n",
       " 'difficult': 45,\n",
       " 'infeasible': 46,\n",
       " 'variety': 47,\n",
       " 'study': 48,\n",
       " 'automatically': 49,\n",
       " 'seen': 50,\n",
       " 'known': 51,\n",
       " 'model': 52,\n",
       " 'artificial': 53,\n",
       " 'wide': 54,\n",
       " 'such': 55,\n",
       " 'computer': 56,\n",
       " 'make': 57,\n",
       " 'and': 58,\n",
       " 'in': 59}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a3894-841f-4dfd-8475-05eef97b7356",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "Next, we will need to convert the text into vectors as input to the neural network. Let's use the one hot encoding feature for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2454c86-c997-424e-ac1c-d47623536ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res\n",
    "\n",
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        idx = concat(\n",
    "            range(max(0, i - window), i), \n",
    "            range(i, min(n_tokens, i + window + 1))\n",
    "        )\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "    \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994daefd-61cd-416d-a129-1c3c5761770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1adb1a52-9d20-4111-a329-3d8c1f13b4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((330, 60), (330, 60))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e798370-8b86-4040-9102-46da32214002",
   "metadata": {},
   "source": [
    "Both X and y are matrices with 330 rows and 60 columns. Here, 330 is the number of training examples we have. We would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. Since, we have one-hot encoded both the input and output as 60-dimensional sparse vectors, this is expected. For example\n",
    "\n",
    "If the sentence is:\n",
    "\"Machine Learning is the study\"\n",
    "\n",
    "Then for the input \"Learning\", the input and output tuples will be\n",
    "(\"Learning\", \"Machine\") \\\n",
    "(\"Learning\", \"is\") \\\n",
    "(\"Learning\", \"the\")\n",
    "\n",
    "Now, we are ready to train our neural network from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc3ca9-48e7-46f7-8d63-6839a4664340",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8ff6ce-2024-47fb-9eb2-4ddd342ef342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    model = {\n",
    "        \"w1\": np.random.randn(vocab_size, n_embedding),\n",
    "        \"w2\": np.random.randn(n_embedding, vocab_size)\n",
    "    }\n",
    "    return model\n",
    "\n",
    "model = init_network(len(word_to_id), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49715a0a-1fef-48ac-9eee-ae4c54c919be",
   "metadata": {},
   "source": [
    "A forward propagation network projects the input vector to the hidden layer after implementing a matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16afcea8-6217-4a14-b5c7-bd757fe30cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X, return_cache=True):\n",
    "    cache = {}\n",
    "    \n",
    "    cache[\"a1\"] = X @ model[\"w1\"]\n",
    "    cache[\"a2\"] = cache[\"a1\"] @ model[\"w2\"]\n",
    "    cache[\"z\"] = softmax(cache[\"a2\"])\n",
    "    \n",
    "    if not return_cache:\n",
    "        return cache[\"z\"]\n",
    "    return cache\n",
    "\n",
    "def softmax(X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x)\n",
    "        res.append(exp / exp.sum())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97d6c78-0ed3-4ab8-99ac-8ec05f496cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X @ model[\"w1\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3b1a69-3c7f-4d0a-af3b-20b92459f6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 60)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X @ model[\"w1\"] @ model[\"w2\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1aa46f-c97e-4fe9-b867-acccb4fc00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, X, y, alpha):\n",
    "    cache  = forward(model, X)\n",
    "    da2 = cache[\"z\"] - y\n",
    "    dw2 = cache[\"a1\"].T @ da2\n",
    "    da1 = da2 @ model[\"w2\"].T\n",
    "    dw1 = X.T @ da1\n",
    "    assert(dw2.shape == model[\"w2\"].shape)\n",
    "    assert(dw1.shape == model[\"w1\"].shape)\n",
    "    model[\"w1\"] -= alpha * dw1\n",
    "    model[\"w2\"] -= alpha * dw2\n",
    "    return cross_entropy(cache[\"z\"], y)\n",
    "\n",
    "def cross_entropy(z, y):\n",
    "    return - np.sum(np.log(z) * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f1c2e8-1967-4d59-9c80-eec21ecc40ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"403.97pt\" height=\"297.190125pt\" viewBox=\"0 0 403.97 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-11-13T08:04:55.906060</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 297.190125 \n",
       "L 403.97 297.190125 \n",
       "L 403.97 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 39.65 273.312 \n",
       "L 396.77 273.312 \n",
       "L 396.77 7.2 \n",
       "L 39.65 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m538cc8d36c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m538cc8d36c\" x=\"55.882727\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(52.701477 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m538cc8d36c\" x=\"122.138757\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(115.776257 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m538cc8d36c\" x=\"188.394787\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(182.032287 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m538cc8d36c\" x=\"254.650816\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(248.288316 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m538cc8d36c\" x=\"320.906846\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(314.544346 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m538cc8d36c\" x=\"387.162876\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(380.800376 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"mb790190754\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"248.984043\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 750 -->\n",
       "      <g transform=\"translate(13.5625 252.783262) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"217.461719\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(7.2 221.260938) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"185.939395\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1250 -->\n",
       "      <g transform=\"translate(7.2 189.738614) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"154.417071\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1500 -->\n",
       "      <g transform=\"translate(7.2 158.21629) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"122.894747\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1750 -->\n",
       "      <g transform=\"translate(7.2 126.693966) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"91.372423\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(7.2 95.171642) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"59.850099\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 2250 -->\n",
       "      <g transform=\"translate(7.2 63.649318) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb790190754\" x=\"39.65\" y=\"28.327775\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 2500 -->\n",
       "      <g transform=\"translate(7.2 32.126994) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 55.882727 19.296 \n",
       "L 62.50833 134.328262 \n",
       "L 69.133933 172.665111 \n",
       "L 75.759536 190.045503 \n",
       "L 82.385139 200.412924 \n",
       "L 89.010742 207.819999 \n",
       "L 95.636345 213.734927 \n",
       "L 102.261948 218.772558 \n",
       "L 108.887551 223.248343 \n",
       "L 115.513154 227.313013 \n",
       "L 122.138757 231.044921 \n",
       "L 128.76436 234.44931 \n",
       "L 135.389963 237.455688 \n",
       "L 142.015566 239.972981 \n",
       "L 148.641169 241.730856 \n",
       "L 155.266772 244.262703 \n",
       "L 161.892375 246.678781 \n",
       "L 168.517978 248.057272 \n",
       "L 175.143581 249.052735 \n",
       "L 181.769184 251.382895 \n",
       "L 188.394787 252.947038 \n",
       "L 195.02039 253.053836 \n",
       "L 201.645993 252.62374 \n",
       "L 208.271596 255.534396 \n",
       "L 214.897199 257.112861 \n",
       "L 221.522801 256.84348 \n",
       "L 228.148404 256.492528 \n",
       "L 234.774007 257.489779 \n",
       "L 241.39961 258.666513 \n",
       "L 248.025213 257.961894 \n",
       "L 254.650816 257.44012 \n",
       "L 261.276419 258.001823 \n",
       "L 267.902022 260.419839 \n",
       "L 274.527625 258.770756 \n",
       "L 281.153228 258.066833 \n",
       "L 287.778831 257.448073 \n",
       "L 294.404434 259.759805 \n",
       "L 301.030037 258.723091 \n",
       "L 307.65564 257.768251 \n",
       "L 314.281243 258.119256 \n",
       "L 320.906846 260.886027 \n",
       "L 327.532449 259.92157 \n",
       "L 334.158052 259.140774 \n",
       "L 340.783655 258.358276 \n",
       "L 347.409258 260.626533 \n",
       "L 354.034861 259.9507 \n",
       "L 360.660464 258.945671 \n",
       "L 367.286067 258.601371 \n",
       "L 373.91167 261.216 \n",
       "L 380.537273 260.177786 \n",
       "\" clip-path=\"url(#pbb9188ea1d)\" style=\"fill: none; stroke: #87ceeb; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 39.65 273.312 \n",
       "L 39.65 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 396.77 273.312 \n",
       "L 396.77 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 39.65 273.312 \n",
       "L 396.77 273.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 39.65 7.2 \n",
       "L 396.77 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pbb9188ea1d\">\n",
       "   <rect x=\"39.65\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "n_iter = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "history = [backward(model, X, y, learning_rate) for _ in range(n_iter)]\n",
    "\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d9236-3d03-493a-baa1-4583d6b156a7",
   "metadata": {},
   "source": [
    "We see that when we try to find surrounding words for the input word \"learning\", the top predicted word corresponds to the word \"machine\" which is a good prediction as per our text. Similarly, when we try to search the same for the word \"machine\", top prediction is \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da71ee3-c1da-4bb4-83eb-9185518b18aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n",
      "is\n",
      "are\n",
      "build\n",
      "intelligence\n",
      "so\n",
      "the\n",
      "to\n",
      "learning\n",
      "subset\n",
      "computer\n",
      "improve\n",
      "a\n",
      "in\n",
      "data\n",
      "algorithms\n",
      "it\n",
      "artificial\n",
      "needed\n",
      "tasks\n",
      "model\n",
      "study\n",
      "such\n",
      "used\n",
      "automatically\n",
      "training\n",
      "do\n",
      "perform\n",
      "make\n",
      "wide\n",
      "on\n",
      "variety\n",
      "based\n",
      "sample\n",
      "applications\n",
      "seen\n",
      "vision\n",
      "filtering\n",
      "experience\n",
      "conventional\n",
      "known\n",
      "of\n",
      "decisions\n",
      "and\n",
      "programmed\n",
      "difficult\n",
      "order\n",
      "where\n",
      "develop\n",
      "mathematical\n",
      "through\n",
      "that\n",
      "email\n",
      "as\n",
      "infeasible\n",
      "being\n",
      "explicitly\n",
      "or\n",
      "predictions\n",
      "without\n"
     ]
    }
   ],
   "source": [
    "learning = one_hot_encode(word_to_id[\"learning\"], len(word_to_id))\n",
    "result = forward(model, [learning], return_cache=False)[0]\n",
    "\n",
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f9fc6c-62de-4591-bba0-0dc066cf6774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n",
      "artificial\n",
      "do\n",
      "so\n",
      "intelligence\n",
      "is\n",
      "filtering\n",
      "algorithms\n",
      "machine\n",
      "to\n",
      "study\n",
      "programmed\n",
      "mathematical\n",
      "subset\n",
      "sample\n",
      "vision\n",
      "such\n",
      "that\n",
      "email\n",
      "needed\n",
      "the\n",
      "applications\n",
      "of\n",
      "explicitly\n",
      "wide\n",
      "are\n",
      "computer\n",
      "difficult\n",
      "a\n",
      "build\n",
      "on\n",
      "improve\n",
      "model\n",
      "make\n",
      "decisions\n",
      "and\n",
      "without\n",
      "training\n",
      "conventional\n",
      "being\n",
      "based\n",
      "infeasible\n",
      "as\n",
      "develop\n",
      "where\n",
      "used\n",
      "tasks\n",
      "through\n",
      "perform\n",
      "seen\n",
      "experience\n",
      "known\n",
      "variety\n",
      "automatically\n",
      "data\n",
      "order\n",
      "it\n",
      "in\n",
      "or\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "machine = one_hot_encode(word_to_id[\"machine\"], len(word_to_id))\n",
    "result = forward(model, [machine], return_cache=False)[0]\n",
    "\n",
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6adf4637-a780-44c1-b5a9-dda40f08bd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0006588435484669411)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a0f44f8-53a8-4bec-bc1d-8280b520ef7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.91381517e-04, 2.84336771e-04, 5.02990238e-05, 1.19343599e-03,\n",
       "       6.87366221e-05, 4.22701159e-04, 2.92138523e-05, 1.02685656e-01,\n",
       "       4.46927632e-03, 1.82620003e-04, 5.82444686e-04, 4.43831557e-01,\n",
       "       6.41986418e-04, 5.85629785e-03, 4.82566836e-05, 2.64834573e-03,\n",
       "       4.01775761e-05, 3.46750186e-05, 6.75954038e-02, 1.67292144e-06,\n",
       "       1.77183866e-04, 1.18643486e-06, 1.96649833e-04, 9.16438392e-05,\n",
       "       5.24494143e-07, 6.58843548e-04, 3.84506505e-04, 2.06282826e-04,\n",
       "       6.20246755e-04, 5.63437553e-02, 2.38355938e-03, 7.45031668e-05,\n",
       "       7.33347163e-04, 6.28360762e-04, 1.52561880e-02, 1.01777304e-04,\n",
       "       1.46917057e-06, 1.46931270e-02, 3.00240954e-03, 1.31383710e-02,\n",
       "       3.97496154e-04, 2.94492315e-06, 1.25475534e-05, 5.01240327e-03,\n",
       "       1.17289829e-01, 5.61676764e-04, 8.55179867e-05, 6.92071729e-06,\n",
       "       5.41688976e-03, 6.71470527e-06, 1.85031445e-05, 7.89890021e-06,\n",
       "       2.63967118e-04, 1.27578039e-01, 5.84852926e-04, 1.48178194e-03,\n",
       "       5.73502424e-04, 2.47521342e-04, 1.97258537e-04, 1.32312491e-06])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb5cee-c01e-4ee6-8850-84365802f40f",
   "metadata": {},
   "source": [
    "To get an embedding for a word, we will need to get the neurons value in the hidden layer. It can be done using the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "314db09d-d522-45e5-bdf3-821ff1b491bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_to_id[word]\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "    one_hot = one_hot_encode(idx, len(word_to_id))\n",
    "    return forward(model, one_hot)[\"a1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d65c5da-bc46-4ae9-8fc1-21417d4a368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.26141668, -0.51379888, -1.2929033 ,  0.44073341,  1.47333039,\n",
       "        0.7165085 , -1.58701711, -0.08694827,  0.5041932 , -0.70820781])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(model, \"machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ea101-b4be-4ef0-adc2-a65deda932e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
