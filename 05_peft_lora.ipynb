{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4750076-4589-45eb-9056-f82111319b69",
   "metadata": {},
   "source": [
    "## NLP - Lesson 5 - LLM based Sentiment Analysis\n",
    "\n",
    "Keeping up with the current trend, LLMs is the hottest thing in the AI industry right now. This notebook will show how to read a pretrained model `GPT2` from the transformers library, use a compatible tokenizer (trained on the same data as the model), generate text after giving an instruction (zero-shot learning), then train the same model using LoRA (Low Rank Adaptation) and PEFT (Parameter Efficient Fine Tuning). Now let's start with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8101989b-c300-4a88-9683-d955a8fa6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "# from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from peft import PeftModel, PeftConfig\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c08da8-b718-4839-8a83-0897edcc46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version :  4.50.3\n",
      "Transformers reinforcement learning version :  0.16.0\n",
      "Peft version :  0.15.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import trl\n",
    "import peft\n",
    "\n",
    "print(\"Transformers version : \",transformers.__version__)\n",
    "print(\"Transformers reinforcement learning version : \",trl.__version__)\n",
    "print(\"Peft version : \",peft.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9d3fe-5112-4a4b-95d5-65f35cec408b",
   "metadata": {},
   "source": [
    "## GPU initiation\n",
    "\n",
    "I will be using my laptop's inbuilt GPU. For windows, I believe it will be `cuda` instead of `mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0f3a6a-5a8c-4c13-a60b-566242f010e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use the MPS backend\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c321c46-0220-495b-8928-5230d7f75dea",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c07db9-a8c4-42c6-9aa9-e65226aefc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n",
      "CPU times: user 157 ms, sys: 99.6 ms, total: 256 ms\n",
      "Wall time: 9.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6a7e1f-13b0-4c05-9a2f-592dab6da56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>1.Eat a balanced diet and make sure to include...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>The three primary colors are red, blue, and ye...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>An atom is made up of a nucleus, which contain...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>There are a number of ways to reduce air pollu...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Describe a time when you had to make a difficu...</td>\n",
       "      <td></td>\n",
       "      <td>I had to make a difficult decision when I was ...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction input  \\\n",
       "0               Give three tips for staying healthy.         \n",
       "1                 What are the three primary colors?         \n",
       "2                 Describe the structure of an atom.         \n",
       "3                   How can we reduce air pollution?         \n",
       "4  Describe a time when you had to make a difficu...         \n",
       "\n",
       "                                              output  \\\n",
       "0  1.Eat a balanced diet and make sure to include...   \n",
       "1  The three primary colors are red, blue, and ye...   \n",
       "2  An atom is made up of a nucleus, which contain...   \n",
       "3  There are a number of ways to reduce air pollu...   \n",
       "4  I had to make a difficult decision when I was ...   \n",
       "\n",
       "                                                text  \n",
       "0  Below is an instruction that describes a task....  \n",
       "1  Below is an instruction that describes a task....  \n",
       "2  Below is an instruction that describes a task....  \n",
       "3  Below is an instruction that describes a task....  \n",
       "4  Below is an instruction that describes a task....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_format = train_dataset.to_pandas()\n",
    "pandas_format.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c8977b-9f1f-44cf-8e1b-bcb406d1af0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instruction    0\n",
       "input          0\n",
       "output         0\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_format.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f872f0fd-cdc7-4510-86f0-1d3120f229ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'Twitter, Instagram, Telegram', '4/16', ...,\n",
       "       'cake, me, eating', 'Michelle Obama',\n",
       "       'The following is an excerpt from a contract between two parties, labeled \"Company A\" and \"Company B\": \\n\\n\"Company A agrees to provide reasonable assistance to Company B in ensuring the accuracy of the financial statements it provides. This includes allowing Company A reasonable access to personnel and other documents which may be necessary for Company B’s review. Company B agrees to maintain the document provided by Company A in confidence, and will not disclose the information to any third parties without Company A’s explicit permission.\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_format['input'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42867a10-a78a-4445-9f4b-41c13212629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction : Give three tips for staying healthy.\n",
      "\n",
      "\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n"
     ]
    }
   ],
   "source": [
    "row = pandas_format.loc[0]\n",
    "\n",
    "example = \"Instruction : \" + row['instruction'] + \"\\n\" + row['input'] + \"\\n\\n\" + row['output'] + \"\\n\\n\" + row['text']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "42797035-ff5d-4d7f-8f96-1d039af890a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output(prompt, model1, tokenizer1, device='mps'):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer1(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = model1.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,         # Enables randomness for creativity\n",
    "        top_k=50,               # Top-k sampling\n",
    "        top_p=0.95,             # Nucleus sampling\n",
    "        temperature=0.1         # Controls randomness (lower = less random)\n",
    "    )\n",
    "    \n",
    "    # Decode and print the result\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Instruction:\", prompt)\n",
    "    print(\"\\nGenerated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "52bb21c4-5f0e-4abe-8a5a-0d03848a000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "\n",
      "Generated Text:\n",
      " Give three tips for staying healthy.\n",
      "\n",
      "1. Don't drink too much.\n",
      "\n",
      "2. Don't drink too much.\n",
      "\n",
      "3. Don't drink too much.\n",
      "\n",
      "\n",
      "4. Don't drink too much.\n",
      "\n",
      "5. Don't drink too much.\n",
      "\n",
      "6. Don't drink too much.\n",
      "\n",
      "\n",
      "7. Don't drink too much.\n",
      "\n",
      "\n",
      "8. Don't drink too much.\n",
      "\n",
      "9.\n",
      "\n",
      "10. Don't drink too much.\n",
      "\n",
      "11. Don't drink too much.\n",
      "\n",
      "12. Don't drink too much.\n",
      "\n",
      "13. Don't drink too much.\n",
      "\n",
      "14. Don't drink too much.\n",
      "\n",
      "15. Don't drink too much.\n",
      "\n",
      "\n",
      "16. Don't drink too much.\n",
      "\n",
      "17. Don't drink too much.\n",
      "\n",
      "18. Don't drink too much.\n",
      "\n",
      "19. Don't drink too much.\n",
      "\n",
      "20. Don't drink too much.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = row['instruction']\n",
    "\n",
    "generate_model_output(prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121e0e8-6bb3-4f04-8e41-011d5d3b6395",
   "metadata": {},
   "source": [
    "We see that even with a lower temperature, the model is generating something that doesn't make sense while repeating majority of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935cc649-084a-4ca1-b87a-f0fa249cb938",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20692c3-9343-4f4e-a7ce-bcb8b011318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mps.empty_cache() For emptying cache if model loading doesn't work properly in one-attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b1334ee-17d1-4f89-ad3f-c285cb62507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initiated!\n",
      "Model downloaded!\n",
      "CPU times: user 2.21 s, sys: 374 ms, total: 2.58 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# pretrained_model_name = \"Salesforce/xgen-7b-8k-base\"\n",
    "# pretrained_model_name = \"microsoft/phi-2\"\n",
    "# pretrained_model_name = \"bigscience/bloom-560m\"\n",
    "# pretrained_model_name = \"sbintuitions/modernbert-ja-30m\"\n",
    "pretrained_model_name = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "print(\"Tokenizer initiated!\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name,\n",
    "#     torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "#     trust_remote_code=True\n",
    "# ).to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "print(\"Model downloaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee629cfe-6c9a-49b1-81bd-4ba2ba1a1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model1):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model1.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    val1 = f\"Trainable model parameters : {trainable_model_params}, \"\n",
    "    val2 = f\"All Model parameters : {all_model_params}, \"\n",
    "    val3 = f\"Percentage trainable parameters : {np.round(100*trainable_model_params/all_model_params,3)}%\"\n",
    "    return val1 + val2 + val3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0af2a776-7e32-468b-8623-8b10ec31af42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trainable model parameters : 124439808, All Model parameters : 124439808, Percentage trainable parameters : 100.0%'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_number_of_trainable_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4cdd27ad-b993-4ce7-be73-58118952bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cebb7f16-ec55-417c-8e40-19cb73175267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_instruction_output(example):\n",
    "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "\n",
    "pandas_format['input_text'] = pandas_format.apply(combine_instruction_output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8c1d6922-05cf-4714-961c-db91010ec903",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(pandas_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "edd3c5a0-af6a-43df-87cc-00f2e3396c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 52002/52002 [00:16<00:00, 3217.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(example, tokenizer1=tokenizer):\n",
    "    # Tokenize the instruction + output as input\n",
    "    model_input = tokenizer1(\n",
    "        example['input_text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # adjust based on your model's context window\n",
    "    )\n",
    "\n",
    "    # Tokenize the label text\n",
    "    with tokenizer1.as_target_tokenizer():\n",
    "        label_output = tokenizer1(\n",
    "            example['text'],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    model_input[\"labels\"] = label_output[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1485791-6489-4356-8b4e-d0e4b76cb32b",
   "metadata": {},
   "source": [
    "### PEFT Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3709010c-3e4b-4efb-839b-223a5806bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_args = TrainingArguments(\n",
    "       output_dir=\"gpt2-fine-tuned\",\n",
    "       per_device_train_batch_size=4,\n",
    "       optim=\"adamw_torch\",\n",
    "       logging_steps=80,\n",
    "       learning_rate=2e-4,\n",
    "       warmup_ratio=0.1,\n",
    "       max_steps = 3,\n",
    "       lr_scheduler_type=\"linear\",\n",
    "       num_train_epochs=1,\n",
    "       save_strategy=\"epoch\",\n",
    "       dataloader_pin_memory=False,\n",
    "       report_to=\"none\"\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "308e7ed9-7a8a-401e-acc7-f475174b0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_peft_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,  # Since we're working with a causal language model\n",
    "#     inference_mode=False,           # Set to False for training\n",
    "#     r=16,                           # Rank of LoRA (reducing parameter count)\n",
    "#     lora_alpha=32,                  # Scaling factor for LoRA weights\n",
    "#     lora_dropout=0.1,               # Dropout rate for LoRA layers\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"]  # Target attention projection layers\n",
    "#     # target_modules=['query_key_value']\n",
    "# )\n",
    "\n",
    "lora_peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Targeting 'query' and 'value' projection layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fee1a39-92dc-4897-91c4-0fe82fc109ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable model parameters : 3244032, All Model parameters : 127683840, Percentage trainable parameters : 2.541%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, lora_peft_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d835c650-9827-4a50-ac91-982dafded275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "peft_trainer = Trainer(\n",
    "       model=peft_model,\n",
    "       train_dataset=tokenized_dataset,\n",
    "       # dataset_text_field=\"text\",\n",
    "       # max_seq_length=1024,\n",
    "       # tokenizer=tokenizer,\n",
    "       args=model_training_args\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5352dc2e-ed5a-4069-aee3-fc4dda0ad265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT complete!\n",
      "CPU times: user 2.29 s, sys: 910 ms, total: 3.2 s\n",
      "Wall time: 4.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-fine-tuned/tokenizer_config.json',\n",
       " './gpt2-fine-tuned/special_tokens_map.json',\n",
       " './gpt2-fine-tuned/vocab.json',\n",
       " './gpt2-fine-tuned/merges.txt',\n",
       " './gpt2-fine-tuned/added_tokens.json',\n",
       " './gpt2-fine-tuned/tokenizer.json')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "peft_trainer.train()\n",
    "\n",
    "print(\"PEFT complete!\")\n",
    "\n",
    "peft_model_path = \"./gpt2-fine-tuned\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ed2f71e-be6d-4c5d-ab21-daba0499906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and  tokenizer downloaded!\n",
      "CPU times: user 2.72 s, sys: 418 ms, total: 3.14 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "peft_model_base = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "print(\"Model and  tokenizer downloaded!\")\n",
    "\n",
    "peft_model_loaded = PeftModel.from_pretrained(peft_model_base,\n",
    "                                      peft_model_path,\n",
    "                                      torch_dtype=torch.bfloat16,\n",
    "                                      is_trainable=False\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68b1fac0-c966-42ab-9179-bc9ac6087928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "\n",
      "Generated Text:\n",
      " Give three tips for staying healthy.\n",
      "\n",
      "1. Get a good diet.\n",
      "\n",
      "If you're not eating enough, you're not going to be able to eat enough.\n",
      "\n",
      "If you're not eating enough, you're not going to be able to eat enough.\n",
      "\n",
      "2. Get a good sleep.\n",
      "\n",
      "If you're not sleeping well, you're not going to be able to sleep well.\n",
      "\n",
      "If you're not sleeping well, you're not going to be able to sleep well.\n",
      "\n",
      "3. Get a good diet.\n",
      "\n",
      "If you're not eating enough, you're not going to be able to eat enough.\n",
      "\n",
      "If you're not eating enough, you're not going to be able to eat enough.\n",
      "\n",
      "4. Get a good sleep.\n",
      "\n",
      "If you're not sleeping well, you're not going to be able to sleep well.\n",
      "\n",
      "If you're not sleeping well, you're not going to be able to sleep well.\n",
      "\n",
      "5.\n"
     ]
    }
   ],
   "source": [
    "prompt = row['instruction']\n",
    "\n",
    "generate_model_output(prompt, peft_model_loaded, tokenizer,device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb583a8-fe90-41ae-8eca-b88d2987befc",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We see that peft model's generated text is much better than what the original gpt2 could generate on our dataset. If we train the model longer then we will have even better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d98561-c4d2-4348-9a27-73dbd316cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf ~/.cache/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e55de-a390-4031-a451-93786207899e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
