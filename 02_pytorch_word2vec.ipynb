{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a83c32-80da-4bf3-85d5-6d721e916fef",
   "metadata": {},
   "source": [
    "## NLP - Lesson 3\n",
    "\n",
    "In Natural Language Processing, the idea is to learn patterns and form insights from textual data using a computer. But as computers cannot understand the text directly, we have to convert the text into numerical data which then can be used as an input to traditional and modern models. Machine learning algorithms can handle any dimension of textual data when converted to numerical data using techniques like word embeddings, for example word2vec. This notebook is a continuation of [00_vectorizer.ipynb](https://github.com/Vishaldawar/nlp_learnings/blob/main/01_vectorizer.ipynb) where we discussed implementation of word2vec from scratch using neural network. In this notebook, we will build up on these techniques by implementing a word2vec model using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cbdcef-2e28-41a5-9345-b21603f692a1",
   "metadata": {},
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb584e82-86ba-4567-bf6f-d45cfddef9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60698ea9-5bee-44af-a9eb-1c4de91cadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e6b22c-3e08-4c18-a1ad-35572d2cd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388e75a-bf52-4caf-9905-f48e7674616e",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Word2Vec is an approach to convert textual data into word embeddings. This is done by using contextual words (or words around the input word) and making the machine predict the surrounding words. This is achieved by using a neural network with a hidden layer. The final output of the neural network is a softmax layer with probabilities of words in our corpus to be the next word. The embeddings however are calculated by as the neuron values in the hidden layer.\n",
    "\n",
    "We will use the same example from last notebook for building a pytorch model on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da666f9b-645f-4826-92cf-0bff51bdd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d95bc-6af6-4767-8f4e-8fa9e947550e",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "We need to tokenize the text to get tokens for each. After that, we will create word to index and index to word mapping dictionaries for finding out the right word corresponding to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5051ee5a-c21f-40a1-888e-cf2a7a4a303d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['machine',\n",
       "  'learning',\n",
       "  'study',\n",
       "  'computer',\n",
       "  'algorithms',\n",
       "  'improve',\n",
       "  'automatically',\n",
       "  'experience',\n",
       "  'seen',\n",
       "  'subset',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'algorithms',\n",
       "  'build',\n",
       "  'mathematical',\n",
       "  'model',\n",
       "  'based',\n",
       "  'sample',\n",
       "  'data',\n",
       "  'known',\n",
       "  'training',\n",
       "  'data',\n",
       "  'order',\n",
       "  'make',\n",
       "  'predictions',\n",
       "  'decisions',\n",
       "  'without',\n",
       "  'explicitly',\n",
       "  'programmed',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'algorithms',\n",
       "  'used',\n",
       "  'wide',\n",
       "  'variety',\n",
       "  'applications',\n",
       "  'email',\n",
       "  'filtering',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'difficult',\n",
       "  'infeasible',\n",
       "  'develop',\n",
       "  'conventional',\n",
       "  'algorithms',\n",
       "  'perform',\n",
       "  'needed',\n",
       "  'tasks'],\n",
       " 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    tokens = cleaned_text.split(' ')\n",
    "    return [token for token in tokens if token not in stopwords.words('english')][:-1]\n",
    "    \n",
    "tokens = clean_and_tokenize(text)\n",
    "tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c12d805-eb63-4bf5-8323-cc369a64852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding word to tokens and vice-versa\n",
    "\n",
    "unique_words = set(tokens)\n",
    "word_id = {word:i for (i,word) in enumerate(unique_words)}\n",
    "id_word = {i:word for (i,word) in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02d01dc-96ec-4383-9188-259c7e0a82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating tuples of input and target variables with a window-size of 2\n",
    "window_size = 2\n",
    "\n",
    "def target_context_tuples(tokens, window_size):\n",
    "    context = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        context_words = [t for t in merge(tokens, i, window_size) if t != token]\n",
    "        for c in context_words:\n",
    "            context.append((token, c))\n",
    "    return context\n",
    "\n",
    "\n",
    "def merge(tokens, i, window_size):\n",
    "    left_id = i - window_size if i >= window_size else i - 1 if i != 0 else i\n",
    "    right_id = i + window_size + 1 if i + window_size <= len(tokens) else len(tokens)\n",
    "    return tokens[left_id:right_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b40b603-754e-43c4-a003-a38582f3ad3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine', 'learning'),\n",
       " ('machine', 'study'),\n",
       " ('learning', 'machine'),\n",
       " ('learning', 'study'),\n",
       " ('learning', 'computer'),\n",
       " ('study', 'machine'),\n",
       " ('study', 'learning'),\n",
       " ('study', 'computer'),\n",
       " ('study', 'algorithms'),\n",
       " ('computer', 'learning'),\n",
       " ('computer', 'study'),\n",
       " ('computer', 'algorithms'),\n",
       " ('computer', 'improve'),\n",
       " ('algorithms', 'study'),\n",
       " ('algorithms', 'computer'),\n",
       " ('algorithms', 'improve'),\n",
       " ('algorithms', 'automatically'),\n",
       " ('improve', 'computer'),\n",
       " ('improve', 'algorithms'),\n",
       " ('improve', 'automatically')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_context_pairs = target_context_tuples(tokens, 2)\n",
    "target_context_pairs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628baf78-ccc3-4464-9882-d25214ec80a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>machine</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target   context\n",
       "0   machine  learning\n",
       "1   machine     study\n",
       "2  learning   machine"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(target_context_pairs, columns=[\"target\",\"context\"])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cb559e-6548-4b2c-81a6-5e72fd2fae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987c9783-cfa7-4631-9d60-8833ad6fc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "vocab_size = len(unique_words)\n",
    "token_indexes = [word_id[token] for token in unique_words]\n",
    "# One-Hot encoding\n",
    "encodings = F.one_hot(torch.tensor(token_indexes), num_classes=vocab_size).float()\n",
    "\n",
    "df[\"target_ohe\"] = df[\"target\"].apply(lambda x : encodings[word_id[x]])\n",
    "df[\"context_ohe\"] = df[\"context\"].apply(lambda x : encodings[word_id[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c3c76a-f95c-41f4-8401-a1e89796bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = df[\"context_ohe\"][idx]\n",
    "        target = df[\"target_ohe\"][idx]\n",
    "        return context, target\n",
    "\n",
    "dataset = W2VDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0ab4b3-9278-4b1f-99fe-0afbe5e11522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6992db4b-7206-4692-99c5-e10b2e1365a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(vocab_size, embed_size)\n",
    "        self.linear2 = torch.nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba03013b-ba02-4d5f-aaa0-fe9edd3af17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EMBED_SIZE = 10    ## Hidden layer of 10 neurons\n",
    "model = Word2Vec(vocab_size, EMBED_SIZE)    # Creating model with forward propagation layer\n",
    "model.to(device)\n",
    "LR = 1e-2          # Learning rate of 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()    # Cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)      # Adam Optimizer for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "562cefed-2291-4109-87ee-b59b7fae7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 3.492892026901245\n",
      "Epoch [20/300], Loss: 3.0534464716911316\n",
      "Epoch [30/300], Loss: 2.6664093732833862\n",
      "Epoch [40/300], Loss: 2.319114923477173\n",
      "Epoch [50/300], Loss: 1.8291687667369843\n",
      "Epoch [60/300], Loss: 1.966289460659027\n",
      "Epoch [70/300], Loss: 1.9675762951374054\n",
      "Epoch [80/300], Loss: 1.9375621974468231\n",
      "Epoch [90/300], Loss: 1.9969501197338104\n",
      "Epoch [100/300], Loss: 1.9292181134223938\n",
      "Epoch [110/300], Loss: 1.7863514721393585\n",
      "Epoch [120/300], Loss: 1.8985019326210022\n",
      "Epoch [130/300], Loss: 1.8724974989891052\n",
      "Epoch [140/300], Loss: 1.7275483310222626\n",
      "Epoch [150/300], Loss: 1.9595456719398499\n",
      "Epoch [160/300], Loss: 1.8207825422286987\n",
      "Epoch [170/300], Loss: 1.654215693473816\n",
      "Epoch [180/300], Loss: 1.6827834844589233\n",
      "Epoch [190/300], Loss: 1.9483663439750671\n",
      "Epoch [200/300], Loss: 1.964514434337616\n",
      "Epoch [210/300], Loss: 1.905747652053833\n",
      "Epoch [220/300], Loss: 1.615349531173706\n",
      "Epoch [230/300], Loss: 1.6514672338962555\n",
      "Epoch [240/300], Loss: 1.9785861670970917\n",
      "Epoch [250/300], Loss: 1.6605339646339417\n",
      "Epoch [260/300], Loss: 1.646304875612259\n",
      "Epoch [270/300], Loss: 1.6860148906707764\n",
      "Epoch [280/300], Loss: 1.5408318936824799\n",
      "Epoch [290/300], Loss: 1.5983606576919556\n",
      "Epoch [300/300], Loss: 1.750771164894104\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    # model.train() # no need since model is in train mode by default\n",
    "    for batch, (context, target) in enumerate(dataloader):\n",
    "        context = context.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(context)\n",
    "        loss = loss_fn(pred, target)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()      # Backward propagation\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = running_loss/len(dataloader)\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss}\")\n",
    "\n",
    "    loss_values.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50284d8-c337-4bd4-bc4f-76cad64193b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine', 'algorithms', 'build', 'study', 'computer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = encodings[word_id[\"learning\"]]\n",
    "[id_word[id.item()] for id in torch.argsort(model(word.to(device)), descending=True).squeeze(0)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdff6790-647c-4947-8c2c-5e5a2f9cdbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine ['algorithms', 'learning', 'programmed', 'artificial', 'study']\n",
      "algorithms ['learning', 'wide', 'machine', 'automatically', 'conventional']\n",
      "mathematical ['algorithms', 'build', 'based', 'model', 'mathematical']\n"
     ]
    }
   ],
   "source": [
    "for word in ['machine','algorithms','mathematical']:\n",
    "    word1 = encodings[word_id[word]]\n",
    "    print(word, [id_word[id.item()] for id in torch.argsort(model(word1.to(device)), descending=True).squeeze(0)[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d9236-3d03-493a-baa1-4583d6b156a7",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We see that when we try to find surrounding words for the input word \"learning\", the top 5 predicted words have \"machine\" which is a good prediction as per our text. Similarly, when we try to search the same for the word \"machine\", top prediction is \"learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f1c2e8-1967-4d59-9c80-eec21ecc40ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"394.423125pt\" height=\"297.190125pt\" viewBox=\"0 0 394.423125 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-12-30T19:47:12.606648</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 297.190125 \n",
       "L 394.423125 297.190125 \n",
       "L 394.423125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 273.312 \n",
       "L 387.223125 273.312 \n",
       "L 387.223125 7.2 \n",
       "L 30.103125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m8ae6cd8566\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"46.335852\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(43.154602 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"100.62591\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(94.26341 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"154.915968\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(145.372218 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"209.206026\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(199.662276 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"263.496083\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(253.952333 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"317.786141\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(308.242391 287.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8ae6cd8566\" x=\"372.076199\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(362.532449 287.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path id=\"m92329833bc\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92329833bc\" x=\"30.103125\" y=\"258.939171\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(7.2 262.73839) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92329833bc\" x=\"30.103125\" y=\"205.174703\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(7.2 208.973921) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92329833bc\" x=\"30.103125\" y=\"151.410234\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 2.5 -->\n",
       "      <g transform=\"translate(7.2 155.209453) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92329833bc\" x=\"30.103125\" y=\"97.645765\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 3.0 -->\n",
       "      <g transform=\"translate(7.2 101.444984) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m92329833bc\" x=\"30.103125\" y=\"43.881297\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 3.5 -->\n",
       "      <g transform=\"translate(7.2 47.680515) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path d=\"M 46.335852 19.296 \n",
       "L 47.421653 23.770754 \n",
       "L 48.507455 26.308095 \n",
       "L 49.593256 31.546245 \n",
       "L 50.679057 31.214869 \n",
       "L 51.764858 31.866559 \n",
       "L 52.850659 38.228201 \n",
       "L 53.93646 43.173571 \n",
       "L 55.022262 49.31669 \n",
       "L 56.108063 44.645609 \n",
       "L 57.193864 49.604817 \n",
       "L 58.279665 47.022727 \n",
       "L 59.365466 63.477634 \n",
       "L 60.451267 60.338652 \n",
       "L 61.537068 89.08258 \n",
       "L 62.62287 78.729934 \n",
       "L 63.708671 76.730012 \n",
       "L 64.794472 80.864853 \n",
       "L 65.880273 87.727414 \n",
       "L 66.966074 91.898723 \n",
       "L 68.051875 85.363429 \n",
       "L 69.137677 105.152896 \n",
       "L 70.223478 95.037076 \n",
       "L 71.309279 102.560672 \n",
       "L 72.39508 93.741932 \n",
       "L 73.480881 116.229336 \n",
       "L 74.566682 100.046904 \n",
       "L 76.738285 156.106138 \n",
       "L 77.824086 133.516411 \n",
       "L 79.995688 154.41393 \n",
       "L 81.081489 154.557592 \n",
       "L 82.16729 121.065025 \n",
       "L 83.253092 135.252272 \n",
       "L 84.338893 159.261232 \n",
       "L 85.424694 165.910155 \n",
       "L 86.510495 181.309598 \n",
       "L 87.596296 185.205573 \n",
       "L 88.682097 170.860614 \n",
       "L 89.767898 158.00155 \n",
       "L 90.8537 173.214728 \n",
       "L 91.939501 196.591232 \n",
       "L 93.025302 189.147278 \n",
       "L 94.111103 180.251685 \n",
       "L 95.196904 190.858438 \n",
       "L 96.282705 174.77954 \n",
       "L 97.368507 189.867174 \n",
       "L 98.454308 180.863849 \n",
       "L 99.540109 223.544004 \n",
       "L 100.62591 172.92291 \n",
       "L 101.711711 185.830107 \n",
       "L 102.797512 190.901534 \n",
       "L 103.883314 220.34042 \n",
       "L 104.969115 196.641769 \n",
       "L 106.054916 201.596426 \n",
       "L 107.140717 197.69858 \n",
       "L 108.226518 191.005286 \n",
       "L 109.312319 178.333083 \n",
       "L 110.39812 208.799561 \n",
       "L 111.483922 197.063108 \n",
       "L 112.569723 195.719879 \n",
       "L 113.655524 181.124294 \n",
       "L 114.741325 202.645857 \n",
       "L 115.827126 202.534878 \n",
       "L 116.912927 202.543998 \n",
       "L 117.998729 194.232478 \n",
       "L 119.08453 181.964159 \n",
       "L 120.170331 206.570503 \n",
       "L 121.256132 208.661189 \n",
       "L 122.341933 202.103797 \n",
       "L 123.427734 223.671246 \n",
       "L 124.513535 200.613162 \n",
       "L 125.599337 209.395725 \n",
       "L 126.685138 186.442498 \n",
       "L 127.770939 228.086987 \n",
       "L 128.85674 196.80838 \n",
       "L 129.942541 228.384429 \n",
       "L 131.028342 230.311728 \n",
       "L 132.114144 211.888573 \n",
       "L 133.199945 210.627049 \n",
       "L 134.285746 189.254196 \n",
       "L 135.371547 208.755603 \n",
       "L 136.457348 192.004939 \n",
       "L 137.543149 197.547348 \n",
       "L 138.62895 209.07891 \n",
       "L 139.714752 234.286507 \n",
       "L 140.800553 223.025295 \n",
       "L 141.886354 218.413974 \n",
       "L 142.972155 205.502653 \n",
       "L 144.057956 217.136971 \n",
       "L 145.143757 214.285206 \n",
       "L 146.229559 213.142458 \n",
       "L 147.31536 235.030442 \n",
       "L 148.401161 226.80128 \n",
       "L 149.486962 213.909847 \n",
       "L 150.572763 224.843692 \n",
       "L 151.658564 198.900194 \n",
       "L 152.744365 224.872408 \n",
       "L 153.830167 212.785804 \n",
       "L 154.915968 216.111415 \n",
       "L 156.001769 223.276345 \n",
       "L 157.08757 217.868232 \n",
       "L 158.173371 214.494564 \n",
       "L 159.259172 204.740971 \n",
       "L 160.344974 212.754533 \n",
       "L 161.430775 232.214828 \n",
       "L 162.516576 206.768065 \n",
       "L 163.602377 223.190125 \n",
       "L 164.688178 228.148102 \n",
       "L 165.773979 211.981837 \n",
       "L 166.859781 216.785544 \n",
       "L 167.945582 222.415137 \n",
       "L 169.031383 220.320481 \n",
       "L 170.117184 220.412155 \n",
       "L 171.202985 221.191136 \n",
       "L 172.288786 245.921579 \n",
       "L 173.374587 227.352169 \n",
       "L 174.460389 232.301164 \n",
       "L 175.54619 216.088682 \n",
       "L 176.631991 219.081568 \n",
       "L 177.717792 219.233477 \n",
       "L 178.803593 220.132774 \n",
       "L 179.889394 223.470294 \n",
       "L 180.975196 213.633776 \n",
       "L 182.060997 229.452683 \n",
       "L 183.146798 242.867318 \n",
       "L 184.232599 226.953345 \n",
       "L 185.3184 224.12566 \n",
       "L 186.404201 218.884911 \n",
       "L 187.490002 216.227826 \n",
       "L 188.575804 239.891669 \n",
       "L 189.661605 204.391963 \n",
       "L 190.747406 232.981939 \n",
       "L 191.833207 224.106381 \n",
       "L 192.919008 225.826578 \n",
       "L 194.004809 233.138215 \n",
       "L 195.090611 219.228045 \n",
       "L 196.176412 242.890484 \n",
       "L 197.262213 234.471141 \n",
       "L 198.348014 230.047245 \n",
       "L 199.433815 216.699388 \n",
       "L 200.519616 245.110084 \n",
       "L 201.605417 241.116167 \n",
       "L 202.691219 238.045556 \n",
       "L 203.77702 239.028106 \n",
       "L 204.862821 238.223245 \n",
       "L 205.948622 242.309209 \n",
       "L 207.034423 244.859106 \n",
       "L 208.120224 209.524713 \n",
       "L 209.206026 228.56706 \n",
       "L 210.291827 236.596046 \n",
       "L 211.377628 241.836365 \n",
       "L 212.463429 228.097783 \n",
       "L 213.54923 208.579254 \n",
       "L 214.635031 218.5974 \n",
       "L 215.720833 241.781621 \n",
       "L 216.806634 229.368418 \n",
       "L 217.892435 253.013715 \n",
       "L 218.978236 224.445765 \n",
       "L 220.064037 220.438279 \n",
       "L 221.149838 218.10678 \n",
       "L 222.235639 234.520652 \n",
       "L 223.321441 244.200023 \n",
       "L 224.407242 226.22213 \n",
       "L 225.493043 238.564985 \n",
       "L 226.578844 232.238725 \n",
       "L 227.664645 224.256129 \n",
       "L 228.750446 227.491272 \n",
       "L 229.836248 242.356522 \n",
       "L 230.922049 225.978967 \n",
       "L 232.00785 254.323661 \n",
       "L 233.093651 221.85292 \n",
       "L 234.179452 239.094284 \n",
       "L 235.265253 239.399225 \n",
       "L 236.351054 213.322121 \n",
       "L 237.436856 248.67619 \n",
       "L 238.522657 219.912329 \n",
       "L 239.608458 237.818586 \n",
       "L 240.694259 239.284657 \n",
       "L 241.78006 228.32341 \n",
       "L 242.865861 214.933137 \n",
       "L 243.951663 225.512924 \n",
       "L 245.037464 242.062405 \n",
       "L 246.123265 238.998393 \n",
       "L 247.209066 245.163925 \n",
       "L 248.294867 217.920124 \n",
       "L 250.466469 251.95544 \n",
       "L 251.552271 210.726815 \n",
       "L 252.638072 235.082091 \n",
       "L 253.723873 222.280813 \n",
       "L 254.809674 232.872584 \n",
       "L 255.895475 236.190492 \n",
       "L 256.981276 258.311678 \n",
       "L 258.067078 215.120767 \n",
       "L 259.152879 234.719883 \n",
       "L 260.23868 201.26629 \n",
       "L 261.324481 218.475573 \n",
       "L 262.410282 208.990428 \n",
       "L 263.496083 250.418287 \n",
       "L 264.581885 232.784749 \n",
       "L 265.667686 238.815646 \n",
       "L 266.753487 224.189063 \n",
       "L 267.839288 248.111114 \n",
       "L 268.925089 233.96079 \n",
       "L 270.01089 259.159227 \n",
       "L 271.096691 234.80121 \n",
       "L 272.182493 228.114001 \n",
       "L 273.268294 215.309557 \n",
       "L 274.354095 222.891189 \n",
       "L 275.439896 234.627574 \n",
       "L 276.525697 222.072878 \n",
       "L 277.611498 234.137079 \n",
       "L 278.6973 238.635066 \n",
       "L 279.783101 216.229316 \n",
       "L 280.868902 261.216 \n",
       "L 281.954703 235.826368 \n",
       "L 283.040504 239.902882 \n",
       "L 284.126305 246.535759 \n",
       "L 285.212106 197.515462 \n",
       "L 286.297908 233.864066 \n",
       "L 287.383709 235.293787 \n",
       "L 288.46951 226.946388 \n",
       "L 289.555311 235.45555 \n",
       "L 290.641112 232.250803 \n",
       "L 291.726913 236.834251 \n",
       "L 292.812715 228.839212 \n",
       "L 293.898516 241.639759 \n",
       "L 294.984317 242.652061 \n",
       "L 296.070118 257.537657 \n",
       "L 297.155919 229.044409 \n",
       "L 298.24172 224.214514 \n",
       "L 299.327521 214.136955 \n",
       "L 300.413323 232.00799 \n",
       "L 301.499124 244.06814 \n",
       "L 302.584925 228.649191 \n",
       "L 303.670726 246.105386 \n",
       "L 304.756527 244.134437 \n",
       "L 305.842328 207.477309 \n",
       "L 306.92813 256.770985 \n",
       "L 308.013931 235.681968 \n",
       "L 309.099732 233.989433 \n",
       "L 310.185533 232.976632 \n",
       "L 311.271334 243.132746 \n",
       "L 312.357135 241.909334 \n",
       "L 313.442936 212.927508 \n",
       "L 314.528738 224.758164 \n",
       "L 315.614539 242.295929 \n",
       "L 316.70034 241.677125 \n",
       "L 317.786141 223.475659 \n",
       "L 318.871942 238.080851 \n",
       "L 319.957743 233.157878 \n",
       "L 321.043545 241.948222 \n",
       "L 322.129346 256.882438 \n",
       "L 323.215147 247.01954 \n",
       "L 324.300948 227.347407 \n",
       "L 325.386749 220.461673 \n",
       "L 326.47255 243.477658 \n",
       "L 327.558352 243.207163 \n",
       "L 328.644153 213.013962 \n",
       "L 329.729954 230.876723 \n",
       "L 330.815755 250.973765 \n",
       "L 331.901556 232.34217 \n",
       "L 332.987357 237.782758 \n",
       "L 334.073158 248.969931 \n",
       "L 335.15896 243.806586 \n",
       "L 336.244761 234.626328 \n",
       "L 337.330562 229.452122 \n",
       "L 338.416363 238.937188 \n",
       "L 339.502164 215.173018 \n",
       "L 340.587965 215.339178 \n",
       "L 341.673767 240.515738 \n",
       "L 342.759568 245.363736 \n",
       "L 343.845369 229.010065 \n",
       "L 344.93117 235.158674 \n",
       "L 346.016971 240.616302 \n",
       "L 347.102772 261.148078 \n",
       "L 348.188573 247.131906 \n",
       "L 349.274375 254.548561 \n",
       "L 350.360176 239.483177 \n",
       "L 351.445977 205.772148 \n",
       "L 352.531778 241.156548 \n",
       "L 353.617579 232.426957 \n",
       "L 354.70338 225.897868 \n",
       "L 355.789182 232.236283 \n",
       "L 356.874983 237.273619 \n",
       "L 357.960784 239.519469 \n",
       "L 359.046585 237.663287 \n",
       "L 360.132386 248.362554 \n",
       "L 361.218187 234.694236 \n",
       "L 362.303988 250.035669 \n",
       "L 363.38979 250.106475 \n",
       "L 364.475591 242.156297 \n",
       "L 365.561392 219.14078 \n",
       "L 366.647193 241.660509 \n",
       "L 367.732994 221.273744 \n",
       "L 368.818795 244.807899 \n",
       "L 369.904597 254.147606 \n",
       "L 370.990398 231.974014 \n",
       "L 370.990398 231.974014 \n",
       "\" clip-path=\"url(#p18dd674265)\" style=\"fill: none; stroke: #87ceeb; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 273.312 \n",
       "L 30.103125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 387.223125 273.312 \n",
       "L 387.223125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 273.312 \n",
       "L 387.223125 273.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.2 \n",
       "L 387.223125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p18dd674265\">\n",
       "   <rect x=\"30.103125\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "plt.plot(range(len(loss_values)), loss_values, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27af5ad-d5cf-448b-a73a-cb54b9cf60b0",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb5cee-c01e-4ee6-8850-84365802f40f",
   "metadata": {},
   "source": [
    "To get an embedding for a word, we will need to get the neurons value in the hidden layer. It can be done using the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b58ac550-26eb-47da-852d-59b6fedab36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3328,  0.2982, -0.8266, -0.4253,  0.8422,  0.2680,  0.3761, -1.3822,\n",
       "         0.0498,  1.2050])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_embedding(model, word):\n",
    "    embeddings = model.linear2.weight.detach().cpu()\n",
    "    id = word_id[word]\n",
    "    return embeddings[id]\n",
    "\n",
    "get_word_embedding(model, \"machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ea101-b4be-4ef0-adc2-a65deda932e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
